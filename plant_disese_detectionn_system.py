# -*- coding: utf-8 -*-
"""plant disese detectionn system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nzJJ6-Xk_Atlhfv0xa1IV4TI3BX8Ib6Q
"""

#mount the drive here

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os
#define the path to the zip file and the extraction directory
zip_file_path = "/content/drive/MyDrive/archive (2).zip"
extract_dir = "/content"
#check if the zip file exists
if os.path.exists(zip_file_path):
    try:
        #create a zipfile oject
        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
           #extract all contents to the specified directory
           zip_ref.extractall(extract_dir)
        print(f"Successfully extracted '{zip_file_path}' to '{extract_dir}'")
    except zipfile.BadZipFile:
       print(f"error:'{zip_file_path}' is not a valid zip file. ")
    except Exception as e:
       print(f"An error occured: {e}")
else:
   print(f"error: '{zip_file_path}' does not exist")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pathlib
import os
import glob as gb
import glob
import cv2
import tensorflow as tf

train= '/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train'

size=224

train_generator = tf.keras.preprocessing.image.ImageDataGenerator(
    rotation_range=90,
    width_shift_range=0.0,
    height_shift_range=0.0,
    shear_range=0.0,
    zoom_range=0.0,
    horizontal_flip=False,
    vertical_flip=False,
    rescale=1/255.0,
    preprocessing_function=None,  # fixed spelling: "preprocessing"
    validation_split=0.1
).flow_from_directory(
    train,  # ensure 'train' is a valid directory path
    batch_size=164,
    target_size=(size, size),
    subset='training',
    color_mode='rgb',  # "rgb", "rgba", or "grayscale"
    class_mode='categorical',  # 'binary', 'sparse', 'categorical', or None
    shuffle=True
)

train_generator.class_indices

classes=list(train_generator.class_indices.keys())
plt.figure(figsize=(20,20))
for X_batch,y_batch in train_generator:
  #create a grid of 3x3 images
  for i in range(0,16):
    plt.subplot(4,4,i+1)
    plt.imshow(X_batch[i])
    plt.title(classes[np.where(y_batch[i]==1)[0] [0]])# when y is categorical
    #plt.tittle(classes[int(y_batch[i])])  #when y is binary or sparse
    plt.grid(None)
  #show the plot
  plt.show()
  break

valid='/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid'

from random import shuffle
valid_generator=tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1/255.0,
    preprocessing_function=None,
    validation_split=0.1
).flow_from_directory(
                   valid,
                  batch_size=164,
                  target_size=(size,size),
                  subset='validation',
                  color_mode='rgb',
                  class_mode='categorical',
                  shuffle=False)

test='/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid'

test_generator=tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    preprocessing_function=None,
  ).flow_from_directory(test,
                        batch_size=164,
                        target_size=(224,224),
                        color_mode='rgb',
                        class_mode='categorical',
                        shuffle=False)

test_generator.class_indices

from tensorflow import keras
model = keras.models.Sequential()  # To build NN

model.add(keras.layers.Conv2D(filters=32, kernel_size=7, strides=1,
                padding="same", activation="relu", name="Conv1", input_shape= (224,224,3)))

model.add(keras.layers.MaxPool2D(pool_size=2, name="Pool1"))

model.add(keras.layers.Conv2D(filters=64, kernel_size=5, strides=1,
                              padding="same", activation="relu", name="Conv2"))
model.add(keras.layers.MaxPool2D(pool_size=2, name="Pool2"))

model.add(keras.layers.Conv2D(filters=128, kernel_size=3, strides=1,
                              padding="same", activation="relu", name="Conv3"))

model.add(keras.layers.Conv2D(filters=256, kernel_size=3, strides=1,
                              padding="same", activation="relu", name="Conv4"))

model.add(keras.layers.MaxPool2D(pool_size=2, name="Pool3"))

model.add(keras.layers.Flatten(name="Flatten1"))   #flatten layer - to convert into 1d vector

model.add(keras.layers.Dense(128, activation="relu", name="Dense1"))  #hidden layer
tf.keras.layers.Dropout(0.5)

model.add(keras.layers.Dense(64, activation="relu", name="Dense2"))  #hidden layer
tf.keras.layers.Dropout(0.5)

model.add(keras.layers.Dense(38, activation="softmax", name="Output"))  #output layer

# The model’s summary() method displays all the model’s layers
print(model.summary())

from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau
early_stopping= EarlyStopping(monitor='val_loss',patience=15,restore_best_weights=True)
model_checkpoint=ModelCheckpoint('best_model.keras',monitor='val_loss',save_best_only=True)
model_ReduceLROnplateau=ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=15,min_lr=0.000001)

callbacks=[early_stopping,model_checkpoint,model_ReduceLROnplateau]

model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy','precision','recall'])

history=model.fit(train_generator,epochs=5, validation_data=valid_generator,callbacks=callbacks)

import seaborn as sns
acc=history.history['accuracy']
val_acc=history.history['val_accuracy']

precision=history.history['precision']
val_precision=history.history['val_precision']

recall=history.history['recall']
val_recall=history.history['val_recall']

loss=history.history['loss']
val_loss=history.history['val_loss']

epochs=range(1,len(loss)+1)

plt.plot(epochs,acc,color='green',label='Training Accuracy')
plt.plot(epochs,val_acc,color='blue',label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.ylim(0, 1.02)
plt.show()

model_evaluate=model.evaluate(test_generator)
print("loss:",model_evaluate[0])
print("Accuracy:",model_evaluate[1])
print("Precision:",model_evaluate[2])
print("recall:",model_evaluate[3])

#to save the model
model.save('PDDS.keras')